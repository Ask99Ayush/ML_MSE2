{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXdXflH5AFhq"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "#  MACHINE LEARNING PIPELINE – EXTREMELY CLEAN + GOD-LEVEL COMMENTS\n",
        "#  This code is written in a generalised way so you can reuse it\n",
        "#  for ANY classification dataset by changing file paths + target.\n",
        "# ================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ================================================================\n",
        "# 1. LOAD DATA\n",
        "#    ✔ Change file paths when using a new dataset.\n",
        "# ================================================================\n",
        "train = pd.read_csv(\"/kaggle/input/mock-test-2-mse-2/train.csv\")\n",
        "test = pd.read_csv(\"/kaggle/input/mock-test-2-mse-2/test.csv\")\n",
        "\n",
        "# ================================\n",
        "# 2. BASIC EDA CHECKS\n",
        "# ================================\n",
        "print(\"\\n===== HEAD OF DATA =====\")\n",
        "print(train.head())\n",
        "\n",
        "print(\"\\n===== DATA INFO =====\")\n",
        "print(train.info())\n",
        "\n",
        "print(\"\\n===== NULL COUNT =====\")\n",
        "print(train.isnull().sum())\n",
        "\n",
        "print(\"\\n===== TARGET DISTRIBUTION =====\")\n",
        "print(train['Status'].value_counts())\n",
        "# NOTE: When using a new dataset → replace \"Status\" with your new target column name.\n",
        "\n",
        "# ================================================================\n",
        "# 3. SEPARATE FEATURES & TARGET\n",
        "# ================================================================\n",
        "y = train[\"Status\"]                     # TARGET COLUMN → change for new dataset\n",
        "X = train.drop(\"Status\", axis=1)        # DROP TARGET FROM FEATURES\n",
        "\n",
        "# Detect column types (helps generalize code)\n",
        "num_cols = X.select_dtypes(include=['int64','float64']).columns\n",
        "cat_cols = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "print(\"\\nNumeric Columns:\", list(num_cols))\n",
        "print(\"Categorical Columns:\", list(cat_cols))\n",
        "\n",
        "# ================================================================\n",
        "# 4. CATEGORY-WISE OUTLIER CAPPING\n",
        "#    ✔ Highly robust method: caps outliers inside each category.\n",
        "#    ✔ Works perfectly for imbalanced or grouped datasets.\n",
        "# ================================================================\n",
        "def cap_outliers_categorywise_all(df, cat_col, num_cols):\n",
        "    \"\"\"\n",
        "    Caps numeric feature outliers separately for each category.\n",
        "    This is useful when distribution varies between categories.\n",
        "\n",
        "    HOW TO REUSE:\n",
        "    - df      : dataset\n",
        "    - cat_col : any categorical column\n",
        "    - num_cols: list of numeric columns\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    for col in num_cols:\n",
        "        Q1 = df.groupby(cat_col)[col].transform(lambda x: x.quantile(0.25))\n",
        "        Q3 = df.groupby(cat_col)[col].transform(lambda x: x.quantile(0.75))\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        lower = Q1 - 1.5 * IQR\n",
        "        upper = Q3 + 1.5 * IQR\n",
        "\n",
        "        df[col] = df[col].clip(lower, upper)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply outlier capping on ALL categorical columns\n",
        "for c in cat_cols:\n",
        "    X = cap_outliers_categorywise_all(X, c, num_cols)\n",
        "    test = cap_outliers_categorywise_all(test, c, num_cols)\n",
        "\n",
        "# ================================================================\n",
        "# 5. VISUALIZATION – EASY MARKS IN EXAM\n",
        "# ================================================================\n",
        "plt.figure(figsize=(10,4))\n",
        "train.isnull().sum().plot(kind='bar')\n",
        "plt.title(\"Missing Values per Column\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.countplot(x=train['Status'])\n",
        "plt.title(\"Target Class Distribution\")\n",
        "plt.show()\n",
        "\n",
        "X[num_cols].hist(figsize=(14,10))\n",
        "plt.suptitle(\"Numeric Feature Distributions\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(X[num_cols].corr(), cmap='coolwarm', annot=False)\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# ================================================================\n",
        "# 6. HANDLE NULL VALUES (VERY IMPORTANT FOR MODEL STABILITY)\n",
        "# ================================================================\n",
        "# For numeric → median (safe for skewed data)\n",
        "X[num_cols] = X[num_cols].fillna(X[num_cols].median())\n",
        "test[num_cols] = test[num_cols].fillna(test[num_cols].median())\n",
        "\n",
        "# For categorical → mode (most common class)\n",
        "X[cat_cols] = X[cat_cols].fillna(X[cat_cols].mode().iloc[0])\n",
        "test[cat_cols] = test[cat_cols].fillna(test[cat_cols].mode().iloc[0])\n",
        "\n",
        "# Reset index to avoid ID issues\n",
        "X = X.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)\n",
        "\n",
        "# ================================================================\n",
        "# 7. LABEL ENCODE TARGET COLUMN\n",
        "#    ✔ Converts labels (strings) → numeric\n",
        "# ================================================================\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "print(\"\\nLabel Mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n",
        "\n",
        "# ================================================================\n",
        "# 8. ONE-HOT ENCODING + PIPELINE\n",
        "# ================================================================\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Preprocessing = OHE for categorical + passthrough numeric\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "        (\"num\", \"passthrough\", num_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Full model pipeline\n",
        "model = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"clf\", RandomForestClassifier(\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "# ================================================================\n",
        "# 9. TRAIN-VALIDATION SPLIT\n",
        "# ================================================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ================================================================\n",
        "# 10. TRAIN MODEL\n",
        "# ================================================================\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ================================================================\n",
        "# 11. EVALUATION METRICS\n",
        "# ================================================================\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "y_pred = model.predict(X_val)\n",
        "y_prob = model.predict_proba(X_val)\n",
        "\n",
        "print(\"\\n========= MODEL PERFORMANCE =========\")\n",
        "print(\"Accuracy :\", accuracy_score(y_val, y_pred))\n",
        "print(\"Precision:\", precision_score(y_val, y_pred, average='macro'))\n",
        "print(\"Recall   :\", recall_score(y_val, y_pred, average='macro'))\n",
        "print(\"F1 Score :\", f1_score(y_val, y_pred, average='macro'))\n",
        "print(\"ROC AUC  :\", roc_auc_score(y_val, y_prob, multi_class='ovr'))\n",
        "\n",
        "# ================================================================\n",
        "# 12. TRAIN FINAL MODEL ON FULL DATA\n",
        "# ================================================================\n",
        "model.fit(X, y_encoded)\n",
        "\n",
        "# ================================================================\n",
        "# 13. GENERATE FINAL PREDICTIONS\n",
        "# ================================================================\n",
        "test_prob = model.predict_proba(test)\n",
        "\n",
        "# NOTE:\n",
        "#  Kaggle expects prediction probabilities class-wise.\n",
        "#  The order of columns must match the encoded labels.\n",
        "# ================================================================\n",
        "\n",
        "submission = pd.DataFrame()\n",
        "submission[\"id\"] = test[\"id\"]\n",
        "\n",
        "# Add probability for each class\n",
        "for class_label in le.classes_:\n",
        "    submission[f\"Status_{class_label}\"] = test_prob[:, le.transform([class_label])[0]]\n",
        "\n",
        "print(\"\\nDuplicate IDs in Submission:\", submission[\"id\"].duplicated().sum())\n",
        "\n",
        "# ================================================================\n",
        "# 14. SAVE FINAL SUBMISSION\n",
        "# ================================================================\n",
        "submission.to_csv(\"submission1.csv\", index=False)\n",
        "print(\"\\nsubmission1.csv CREATED SUCCESSFULLY!\\n\")\n",
        "print(submission.head())\n"
      ]
    }
  ]
}