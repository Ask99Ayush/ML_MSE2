{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBsfNU-C69-0"
      },
      "outputs": [],
      "source": [
        "# =====================================================================\n",
        "# üìå COMPLETE PIPELINE WITH EDA + OUTLIER ANALYSIS + RANDOM FOREST\n",
        "# =====================================================================\n",
        "\n",
        "# 1Ô∏è‚É£ Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_auc_score, log_loss, confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# =====================================================================\n",
        "# 2Ô∏è‚É£ Load Data\n",
        "# =====================================================================\n",
        "train = pd.read_csv(\"/content/train.csv\")\n",
        "test = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "print(\"Train Shape:\", train.shape)\n",
        "print(\"Test Shape:\", test.shape)\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# üé® 3Ô∏è‚É£ PERFORM EDA (Exploratory Data Analysis)\n",
        "# =====================================================================\n",
        "print(\"\\n================ EDA & VISUALIZATION ================\")\n",
        "\n",
        "# --- A. Check for Nulls ---\n",
        "print(\"\\nNull Values per Column:\")\n",
        "print(train.isnull().sum())\n",
        "train.info()\n",
        "train.head()\n",
        "# train.drop(['id','CustomerId','Surname'],axis=1,inplace=True)\n",
        "train.head()\n",
        "train.isnull().sum()\n",
        "train.duplicated().sum()\n",
        "train.nunique()\n",
        "\n",
        "# --- B. Target Variable Distribution ---\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x=train[\"Status\"], palette=\"viridis\")\n",
        "plt.title(\"Distribution of Target Variable (Status)\")\n",
        "plt.show()\n",
        "\n",
        "# --- C. Correlation Heatmap ---\n",
        "# Select only numeric columns for correlation\n",
        "numeric_df = train.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# --- D. Box Plots (Outlier Visualization) ---\n",
        "# We loop through numeric columns to visualize distributions\n",
        "cols_to_plot = numeric_df.columns\n",
        "# Calculate grid size for subplots\n",
        "n_cols = 3\n",
        "n_rows = (len(cols_to_plot) - 1) // n_cols + 1\n",
        "\n",
        "plt.figure(figsize=(15, n_rows * 4))\n",
        "for i, col in enumerate(cols_to_plot):\n",
        "    plt.subplot(n_rows, n_cols, i + 1)\n",
        "    sns.boxplot(x=train[col], color=\"skyblue\")\n",
        "    plt.title(f\"Boxplot of {col}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 4Ô∏è‚É£ Data Cleaning: Drop Nulls (As Requested)\n",
        "# =====================================================================\n",
        "print(\"\\n================ DATA CLEANING ================\")\n",
        "initial_rows = len(train)\n",
        "\n",
        "# Drop rows with ANY missing values in the training set\n",
        "train.dropna(inplace=True)\n",
        "\n",
        "print(f\"Rows dropped: {initial_rows - len(train)}\")\n",
        "print(f\"Remaining Training Rows: {len(train)}\")\n",
        "\n",
        "# NOTE: We CANNOT drop rows from Test data (we must submit all rows).\n",
        "# We will fill Test nulls with Mean/Mode later to prevent errors.\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 5Ô∏è‚É£ Split Target + Features\n",
        "# =====================================================================\n",
        "y = train[\"Status\"]\n",
        "X = train.drop(columns=[\"Status\"])\n",
        "\n",
        "# Safety Fill for TEST data only (Since we can't drop test rows)\n",
        "for col in test.columns:\n",
        "    if test[col].dtype == 'object':\n",
        "        test[col] = test[col].fillna(test[col].mode()[0])\n",
        "    else:\n",
        "        test[col] = test[col].fillna(test[col].mean())\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 6Ô∏è‚É£ Outlier Analysis (Isolation Forest + IQR)\n",
        "# =====================================================================\n",
        "print(\"\\n================ OUTLIER REMOVAL ================\")\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# ------------------------- (A) Isolation Forest ------------------------\n",
        "iso = IsolationForest(contamination=0.03, random_state=42)\n",
        "outlier_flags = iso.fit_predict(X[numeric_cols])\n",
        "\n",
        "# -1 = outlier, 1 = normal\n",
        "X_clean = X[outlier_flags == 1]\n",
        "y_clean = y[outlier_flags == 1]\n",
        "\n",
        "print(f\"Rows removed by IsolationForest: {len(X) - len(X_clean)}\")\n",
        "\n",
        "# ------------------------- (B) IQR Outlier Capping ----------------------\n",
        "def cap_outliers_categorywise_all(df, cat_col, num_cols):\n",
        "    def cap_group(group):\n",
        "        group = group.copy()\n",
        "        for col in num_cols:\n",
        "            Q1 = group[col].quantile(0.25)\n",
        "            Q3 = group[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower = Q1 - 1.5 * IQR\n",
        "            upper = Q3 + 1.5 * IQR\n",
        "            group[col] = group[col].clip(lower, upper)\n",
        "        return group\n",
        "    return df.groupby(cat_col, group_keys=False, observed=True, sort=False).apply(cap_group)\n",
        "\n",
        "# Find a categorical column to group by (or skip if none exist)\n",
        "if len(X_clean.select_dtypes(include=\"object\").columns) > 0:\n",
        "    categorical_col_for_capping = \"Region\" if \"Region\" in X_clean.columns else X_clean.select_dtypes(include=\"object\").columns[0]\n",
        "\n",
        "    # Combine X and y temporarily for the groupby operation\n",
        "    temp_df = X_clean.copy()\n",
        "    temp_df[\"Status\"] = y_clean\n",
        "\n",
        "    X_capped_full = cap_outliers_categorywise_all(temp_df, categorical_col_for_capping, numeric_cols)\n",
        "\n",
        "    y_clean = X_capped_full[\"Status\"]\n",
        "    X_clean = X_capped_full.drop(columns=[\"Status\"])\n",
        "    print(\"IQR Capping complete!\")\n",
        "else:\n",
        "    print(\"No categorical column found for IQR grouping. Skipping IQR step.\")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 7Ô∏è‚É£ Train‚ÄìValidation Split\n",
        "# =====================================================================\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_clean, y_clean, test_size=0.2, random_state=42, stratify=y_clean\n",
        ")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 8Ô∏è‚É£ Preprocessing Pipeline setup\n",
        "# =====================================================================\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
        "        (\"num\", \"passthrough\", numeric_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "base_model = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 9Ô∏è‚É£ Hyperparameter Tuning\n",
        "# =====================================================================\n",
        "print(\"\\n================ MODEL TRAINING ================\")\n",
        "param_grid = {\n",
        "    \"classifier__n_estimators\": [100, 200],\n",
        "    \"classifier__max_depth\": [10, 20],\n",
        "    \"classifier__min_samples_split\": [2, 5],\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    base_model,\n",
        "    param_grid,\n",
        "    scoring=\"accuracy\",  # Changed to accuracy (safer if log_loss fails on some metrics)\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "print(\"\\nüîé Best Parameters:\", grid.best_params_)\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# üîü Validation Results\n",
        "# =====================================================================\n",
        "y_pred = best_model.predict(X_val)\n",
        "y_proba = best_model.predict_proba(X_val)\n",
        "\n",
        "print(\"\\n================ VALIDATION METRICS ================\")\n",
        "print(\"Accuracy :\", accuracy_score(y_val, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_val, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "sns.heatmap(confusion_matrix(y_val, y_pred), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ Submission Generation\n",
        "# =====================================================================\n",
        "test_processed = test[X.columns]\n",
        "predict_proba = best_model.predict_proba(test_processed)\n",
        "class_labels = list(best_model.classes_)\n",
        "\n",
        "submission = pd.DataFrame({\"id\": test[\"id\"]})\n",
        "for i, cls in enumerate(class_labels):\n",
        "    submission[f\"Status_{cls}\"] = predict_proba[:, i]\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"\\nüìÅ Submission saved to submission.csv\")"
      ]
    }
  ]
}