{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBsfNU-C69-0"
      },
      "outputs": [],
      "source": [
        "# =====================================================================\n",
        "# üìå COMPLETE PIPELINE WITH EDA + OUTLIER ANALYSIS + RANDOM FOREST\n",
        "# üìÅ FILE: COMPLETE_EDA_OUTLIER_RF_PIPELINE.py\n",
        "# =====================================================================\n",
        "\n",
        "# ==============================================================\n",
        "# 1Ô∏è‚É£ IMPORT LIBRARIES\n",
        "#    - numpy/pandas ‚Üí data processing\n",
        "#    - seaborn/matplotlib ‚Üí visualization\n",
        "#    - sklearn ‚Üí ML processing + modeling\n",
        "# ==============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_auc_score, log_loss, confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# =====================================================================\n",
        "# 2Ô∏è‚É£ LOAD DATA\n",
        "# =====================================================================\n",
        "train = pd.read_csv(\"/content/train.csv\")   # training data\n",
        "test = pd.read_csv(\"/content/test.csv\")     # test data for submission\n",
        "\n",
        "print(\"Train Shape:\", train.shape)\n",
        "print(\"Test Shape:\", test.shape)\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# üé® 3Ô∏è‚É£ EXPLORATORY DATA ANALYSIS (EDA)\n",
        "# =====================================================================\n",
        "print(\"\\n================ EDA & VISUALIZATION ================\")\n",
        "\n",
        "# --- (A) Check missing values ---\n",
        "print(\"\\nNull Values per Column:\")\n",
        "print(train.isnull().sum())\n",
        "train.info()         # show datatypes + nulls\n",
        "train.head()         # preview rows\n",
        "train.duplicated().sum()   # count duplicate rows\n",
        "train.nunique()      # number of unique values per column\n",
        "\n",
        "# --- (B) Target Distribution ---\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x=train[\"Status\"], palette=\"viridis\")\n",
        "plt.title(\"Distribution of Target Variable (Status)\")\n",
        "plt.show()\n",
        "\n",
        "# --- (C) Correlation Heatmap ---\n",
        "numeric_df = train.select_dtypes(include=['float64', 'int64'])\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# --- (D) Boxplots for Outlier Detection ---\n",
        "cols_to_plot = numeric_df.columns\n",
        "n_cols = 3\n",
        "n_rows = (len(cols_to_plot) - 1) // n_cols + 1\n",
        "\n",
        "plt.figure(figsize=(15, n_rows * 4))\n",
        "for i, col in enumerate(cols_to_plot):\n",
        "    plt.subplot(n_rows, n_cols, i + 1)\n",
        "    sns.boxplot(x=train[col], color=\"skyblue\")\n",
        "    plt.title(f\"Boxplot of {col}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 4Ô∏è‚É£ DATA CLEANING ‚Äì DROP NULLS (Training only)\n",
        "# =====================================================================\n",
        "print(\"\\n================ DATA CLEANING ================\")\n",
        "initial_rows = len(train)\n",
        "\n",
        "# Removes any rows in TRAIN with null values\n",
        "train.dropna(inplace=True)\n",
        "\n",
        "print(f\"Rows dropped: {initial_rows - len(train)}\")\n",
        "print(f\"Remaining Training Rows: {len(train)}\")\n",
        "\n",
        "# Test dataset must keep original row count ‚Üí fill instead of drop\n",
        "for col in test.columns:\n",
        "    if test[col].dtype == 'object':\n",
        "        test[col] = test[col].fillna(test[col].mode()[0])  # fill categorical\n",
        "    else:\n",
        "        test[col] = test[col].fillna(test[col].mean())     # fill numerical\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 5Ô∏è‚É£ SEPARATE FEATURES & TARGET\n",
        "# =====================================================================\n",
        "y = train[\"Status\"]               # target column\n",
        "X = train.drop(columns=[\"Status\"])    # feature columns\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 6Ô∏è‚É£ OUTLIER ANALYSIS (Isolation Forest + IQR Capping)\n",
        "# =====================================================================\n",
        "print(\"\\n================ OUTLIER REMOVAL ================\")\n",
        "\n",
        "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# --- (A) Isolation Forest Outlier Detection ---\n",
        "iso = IsolationForest(contamination=0.03, random_state=42)\n",
        "outlier_flags = iso.fit_predict(X[numeric_cols])  # -1 = outlier\n",
        "\n",
        "X_clean = X[outlier_flags == 1]   # keep only non-outliers\n",
        "y_clean = y[outlier_flags == 1]\n",
        "\n",
        "print(f\"Rows removed by IsolationForest: {len(X) - len(X_clean)}\")\n",
        "\n",
        "\n",
        "# --- (B) IQR Capping (per category group) ---\n",
        "def cap_outliers_categorywise_all(df, cat_col, num_cols):\n",
        "    \"\"\"\n",
        "    Caps numeric column values within each category group based on IQR.\n",
        "    Prevents extreme values while preserving category distribution.\n",
        "    \"\"\"\n",
        "    def cap_group(group):\n",
        "        group = group.copy()\n",
        "        for col in num_cols:\n",
        "            Q1 = group[col].quantile(0.25)\n",
        "            Q3 = group[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower = Q1 - 1.5 * IQR\n",
        "            upper = Q3 + 1.5 * IQR\n",
        "            group[col] = group[col].clip(lower, upper)  # cap outliers\n",
        "        return group\n",
        "    return df.groupby(cat_col, group_keys=False, observed=True, sort=False).apply(cap_group)\n",
        "\n",
        "# Choose first categorical column for grouping\n",
        "cat_columns = X_clean.select_dtypes(include=\"object\").columns\n",
        "\n",
        "if len(cat_columns) > 0:\n",
        "    categorical_col_for_capping = \"Region\" if \"Region\" in X_clean.columns else cat_columns[0]\n",
        "\n",
        "    temp_df = X_clean.copy()\n",
        "    temp_df[\"Status\"] = y_clean\n",
        "\n",
        "    capped_df = cap_outliers_categorywise_all(temp_df, categorical_col_for_capping, numeric_cols)\n",
        "\n",
        "    y_clean = capped_df[\"Status\"]\n",
        "    X_clean = capped_df.drop(columns=[\"Status\"])\n",
        "\n",
        "    print(\"IQR Outlier Capping completed!\")\n",
        "else:\n",
        "    print(\"No categorical column found for IQR method. Skipping...\")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 7Ô∏è‚É£ TRAIN‚ÄìVALIDATION SPLIT\n",
        "# =====================================================================\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_clean, y_clean, test_size=0.2, random_state=42, stratify=y_clean\n",
        ")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 8Ô∏è‚É£ PREPROCESSING PIPELINE\n",
        "#    Combines OneHotEncoding + numeric passthrough\n",
        "# =====================================================================\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
        "        (\"num\", \"passthrough\", numeric_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "base_model = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 9Ô∏è‚É£ GRIDSEARCH HYPERPARAMETER TUNING\n",
        "# =====================================================================\n",
        "print(\"\\n================ MODEL TRAINING ================\")\n",
        "\n",
        "param_grid = {\n",
        "    \"classifier__n_estimators\": [100, 200],\n",
        "    \"classifier__max_depth\": [10, 20],\n",
        "    \"classifier__min_samples_split\": [2, 5],\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    base_model,\n",
        "    param_grid,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "print(\"\\nüîé Best Parameters:\", grid.best_params_)\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# üîü VALIDATION EVALUATION\n",
        "# =====================================================================\n",
        "y_pred = best_model.predict(X_val)\n",
        "y_proba = best_model.predict_proba(X_val)\n",
        "\n",
        "print(\"\\n================ VALIDATION METRICS ================\")\n",
        "print(\"Accuracy :\", accuracy_score(y_val, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_val, y_pred))\n",
        "\n",
        "# Confusion Matrix Plot\n",
        "sns.heatmap(confusion_matrix(y_val, y_pred), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ GENERATE SUBMISSION FILE\n",
        "# =====================================================================\n",
        "test_processed = test[X.columns]   # keep original feature order\n",
        "\n",
        "predict_proba = best_model.predict_proba(test_processed)\n",
        "class_labels = list(best_model.classes_)\n",
        "\n",
        "# Create submission file structure\n",
        "submission = pd.DataFrame({\"id\": test[\"id\"]})\n",
        "for i, cls in enumerate(class_labels):\n",
        "    submission[f\"Status_{cls}\"] = predict_proba[:, i]\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"\\nüìÅ Submission saved to submission.csv\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
