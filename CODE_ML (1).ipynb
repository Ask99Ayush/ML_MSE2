{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sxZEjIQQab8"
      },
      "outputs": [],
      "source": [
        "# ================================================\n",
        "# 1. IMPORT LIBRARIES\n",
        "# ================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ================================================\n",
        "# 2. LOAD DATA\n",
        "# ================================================\n",
        "train = pd.read_csv(\"/kaggle/input/mock-test-2-mse-2/train.csv\")\n",
        "test  = pd.read_csv(\"/kaggle/input/mock-test-2-mse-2/test.csv\")\n",
        "\n",
        "# Look at the data\n",
        "print(train.head())\n",
        "print(train.info())\n",
        "print(train.isnull().sum())\n",
        "print(train['Status'].value_counts())\n",
        "\n",
        "# ================================================\n",
        "# 3. SPLIT FEATURES & TARGET\n",
        "# ================================================\n",
        "y = train[\"Status\"]              # Target column\n",
        "X = train.drop(\"Status\", axis=1) # Input features\n",
        "\n",
        "# Separate numeric & categorical columns\n",
        "num_cols = X.select_dtypes(include=['int64','float64']).columns\n",
        "cat_cols = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# ================================================\n",
        "# 4. HANDLE MISSING VALUES\n",
        "# ================================================\n",
        "# Numeric -> fill with median\n",
        "X[num_cols] = X[num_cols].fillna(X[num_cols].median())\n",
        "test[num_cols] = test[num_cols].fillna(test[num_cols].median())\n",
        "\n",
        "# Categorical -> fill with mode (most frequent value)\n",
        "X[cat_cols] = X[cat_cols].fillna(X[cat_cols].mode().iloc[0])\n",
        "test[cat_cols] = test[cat_cols].fillna(test[cat_cols].mode().iloc[0])\n",
        "\n",
        "# Reset index (safety)\n",
        "X = X.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)\n",
        "\n",
        "# ================================================\n",
        "# 5. LABEL ENCODE TARGET (C, CL, D â†’ 0,1,2)\n",
        "# ================================================\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "print(\"Label Mapping:\", le.classes_)\n",
        "# Output â†’ ['C', 'CL', 'D']\n",
        "\n",
        "# ================================================\n",
        "# 6. ONE-HOT ENCODE + RANDOM FOREST MODEL\n",
        "# ================================================\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Preprocessing for categorical & numeric cols\n",
        "preprocess = ColumnTransformer([\n",
        "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "    (\"num\", \"passthrough\", num_cols)\n",
        "])\n",
        "\n",
        "# Build a pipeline\n",
        "model = Pipeline([\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"clf\", RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# ================================================\n",
        "# 7. TRAIN / VALIDATION SPLIT\n",
        "# ================================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ================================================\n",
        "# 8. TRAIN THE MODEL\n",
        "# ================================================\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ================================================\n",
        "# 9. MODEL EVALUATION\n",
        "# ================================================\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "y_pred = model.predict(X_val)\n",
        "y_prob = model.predict_proba(X_val) # probability for AUC\n",
        "\n",
        "print(\"Accuracy :\", accuracy_score(y_val, y_pred))\n",
        "print(\"Precision:\", precision_score(y_val, y_pred, average='macro'))\n",
        "print(\"Recall   :\", recall_score(y_val, y_pred, average='macro'))\n",
        "print(\"F1 Score :\", f1_score(y_val, y_pred, average='macro'))\n",
        "print(\"ROC AUC  :\", roc_auc_score(y_val, y_prob, multi_class='ovr'))\n",
        "\n",
        "# ================================================\n",
        "# 10. TRAIN ON FULL DATASET\n",
        "# ================================================\n",
        "model.fit(X, y_encoded)\n",
        "\n",
        "# ================================================\n",
        "# 11. PREDICT TEST SET\n",
        "# ================================================\n",
        "test_pred = model.predict_proba(test)\n",
        "\n",
        "# ================================================\n",
        "# 12. CREATE SUBMISSION FILE\n",
        "# ================================================\n",
        "submission = pd.DataFrame({\n",
        "    \"id\": test[\"id\"],\n",
        "    \"Status_C\":  test_pred[:, le.transform(['C'])[0]],\n",
        "    \"Status_CL\": test_pred[:, le.transform(['CL'])[0]],\n",
        "    \"Status_D\":  test_pred[:, le.transform(['D'])[0]],\n",
        "})\n",
        "\n",
        "print(\"Duplicate IDs:\", submission[\"id\"].duplicated().sum())\n",
        "\n",
        "submission.to_csv(\"submission6.csv\", index=False)\n",
        "print(\"submission6.csv CREATED!\")\n",
        "\n",
        "submission.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 1. IMPORT LIBRARIES\n",
        "# ============================================\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# For clean output\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ============================================\n",
        "# 2. LOAD DATA\n",
        "# ============================================\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Titanic-Dataset.csv\")\n",
        "\n",
        "print(\"First 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df.info())\n",
        "\n",
        "# Remove rows where target is missing\n",
        "df = df.dropna(subset=['Survived'])\n",
        "\n",
        "# ============================================\n",
        "# 3. EDA (GRAPHS)\n",
        "# ============================================\n",
        "\n",
        "# --- Missing Values Plot ---\n",
        "plt.figure(figsize=(10,4))\n",
        "df.isnull().sum().plot(kind='bar', color='red')\n",
        "plt.title(\"Missing Values Per Column\")\n",
        "plt.show()\n",
        "\n",
        "# --- Survival Count ---\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x=df['Survived'])\n",
        "plt.title(\"Survival Count (0=No, 1=Yes)\")\n",
        "plt.show()\n",
        "\n",
        "# --- Survival by Gender ---\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='Sex', hue='Survived', data=df)\n",
        "plt.title(\"Survival by Gender\")\n",
        "plt.show()\n",
        "\n",
        "# --- Age Distribution ---\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.histplot(df['Age'], kde=True)\n",
        "plt.title(\"Age Distribution\")\n",
        "plt.show()\n",
        "\n",
        "# --- Correlation Heatmap ---\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# 4. SELECT FEATURES FOR PREDICTION\n",
        "# ============================================\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "X = df[features]\n",
        "y = df['Survived']\n",
        "\n",
        "# ============================================\n",
        "# 5. HANDLE MISSING VALUES\n",
        "# ============================================\n",
        "\n",
        "# Replace missing Age with median\n",
        "X['Age'].fillna(X['Age'].median(), inplace=True)\n",
        "\n",
        "# Replace missing Embarked with most frequent\n",
        "X['Embarked'].fillna(X['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# ============================================\n",
        "# 6. LABEL ENCODING (text â†’ numbers)\n",
        "# ============================================\n",
        "le = LabelEncoder()\n",
        "\n",
        "X['Sex'] = le.fit_transform(X['Sex'])         # male/female â†’ 0/1\n",
        "X['Embarked'] = le.fit_transform(X['Embarked'])  # S, C, Q â†’ 0/1/2\n",
        "\n",
        "# ============================================\n",
        "# 7. TRAIN/TEST SPLIT\n",
        "# ============================================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# 8. TRAIN RANDOM FOREST MODEL\n",
        "# ============================================\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ============================================\n",
        "# 9. PREDICT ON TEST DATA\n",
        "# ============================================\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# ============================================\n",
        "# 10. EVALUATE MODEL\n",
        "# ============================================\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nâœ… Accuracy:\", round(acc * 100, 2), \"%\")\n",
        "\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# ============================================\n",
        "# 11. SAMPLE PREDICTION\n",
        "# ============================================\n",
        "sample = X_test.iloc[0:1]\n",
        "print(\"\\nSample Passenger:\")\n",
        "print(sample)\n",
        "\n",
        "prediction = model.predict(sample)[0]\n",
        "print(\"\\nPredicted (1 = Survived):\", prediction)\n"
      ],
      "metadata": {
        "id": "_-s-WI8ZTDFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# 1. IMPORT LIBRARIES\n",
        "# ===============================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix\n",
        "\n",
        "# ===============================================\n",
        "# 2. LOAD DATA\n",
        "# ===============================================\n",
        "train_df = pd.read_csv('/Users/piyushagarwal/Documents/Python Files/Mock Dataset/train (1).csv')\n",
        "test_df  = pd.read_csv('/Users/piyushagarwal/Documents/Python Files/Mock Dataset/test (1).csv')\n",
        "\n",
        "train_df.info()\n",
        "train_df.head()\n",
        "\n",
        "# ===============================================\n",
        "# 3. DROP USELESS COLUMNS\n",
        "# (ID columns don't help in ML)\n",
        "# ===============================================\n",
        "train_df.drop(['id','CustomerId','Surname'], axis=1, inplace=True)\n",
        "\n",
        "train_df.head()\n",
        "\n",
        "# ===============================================\n",
        "# 4. CHECK BASIC DATA QUALITY\n",
        "# ===============================================\n",
        "train_df.isnull().sum()\n",
        "train_df.duplicated().sum()\n",
        "train_df.nunique()\n",
        "\n",
        "# Target and categorical value counts\n",
        "train_df['Geography'].value_counts()\n",
        "train_df['Gender'].value_counts()\n",
        "train_df['NumOfProducts'].value_counts()\n",
        "train_df['HasCrCard'].value_counts()\n",
        "train_df['IsActiveMember'].value_counts()\n",
        "train_df['Exited'].value_counts()\n",
        "\n",
        "# ===============================================\n",
        "# 5. ONE-HOT ENCODING (Convert categories â†’ numbers)\n",
        "# drop_first removes dummy trap\n",
        "# ===============================================\n",
        "df_encoded = pd.get_dummies(train_df, columns=['Gender', 'Geography'],\n",
        "                            drop_first=True, dtype=int)\n",
        "df_encoded.info()\n",
        "\n",
        "train_df = df_encoded\n",
        "\n",
        "# ===============================================\n",
        "# 6. BOXPLOTS BEFORE OUTLIER REMOVAL\n",
        "# Check numeric columns vs target\n",
        "# ===============================================\n",
        "category_col = 'Exited'\n",
        "numeric_cols = ['CreditScore', 'Age', 'Tenure', 'Balance', 'EstimatedSalary']\n",
        "\n",
        "for col in numeric_cols:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.boxplot(x=category_col, y=col, data=train_df)\n",
        "    plt.title(f'Boxplot of {col} by {category_col}')\n",
        "    plt.show()\n",
        "\n",
        "# ===============================================\n",
        "# 7. REMOVE OUTLIERS USING IQR\n",
        "# ===============================================\n",
        "def remove_outliers_iqr(df, columns):\n",
        "    for col in columns:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower = Q1 - 1.5 * IQR\n",
        "        upper = Q3 + 1.5 * IQR\n",
        "        df = df[(df[col] >= lower) & (df[col] <= upper)]\n",
        "    return df\n",
        "\n",
        "df_clean = remove_outliers_iqr(train_df, numeric_cols)\n",
        "print(\"Before:\", train_df.shape[0], \"rows\")\n",
        "print(\"After :\", df_clean.shape[0], \"rows\")\n",
        "\n",
        "# Boxplots after outlier removal\n",
        "for col in numeric_cols:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.boxplot(x=category_col, y=col, data=df_clean)\n",
        "    plt.title(f'Boxplot of {col} by {category_col}')\n",
        "    plt.show()\n",
        "\n",
        "# ===============================================\n",
        "# 8. CORRELATION HEATMAP\n",
        "# ===============================================\n",
        "corr = train_df.corr(numeric_only=True)\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(corr, annot=True, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation Heatmap\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# ===============================================\n",
        "# 9. PAIRPLOT (Multivariate EDA)\n",
        "# ===============================================\n",
        "sns.pairplot(train_df, vars=['CreditScore', 'Balance', 'Age',\n",
        "                             'EstimatedSalary'],\n",
        "             hue='Exited', diag_kind='kde')\n",
        "plt.show()\n",
        "\n",
        "# ===============================================\n",
        "# 10. DEFINE FEATURES & TARGET\n",
        "# ===============================================\n",
        "X = train_df.drop('Exited', axis=1)\n",
        "y = train_df['Exited']\n",
        "\n",
        "# ===============================================\n",
        "# 11. BALANCE DATA USING SMOTE\n",
        "# (Fix Imbalanced Target)\n",
        "# ===============================================\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "min_class_size = counts.min()\n",
        "k = min(5, max(1, min_class_size - 1))  # dynamic k\n",
        "\n",
        "smote = SMOTE(random_state=42, k_neighbors=k)\n",
        "X_res, y_res = smote.fit_resample(X, y)\n",
        "\n",
        "print(\"Before SMOTE:\", dict(zip(unique, counts)))\n",
        "print(\"After SMOTE :\", dict(zip(*np.unique(y_res, return_counts=True))))\n",
        "\n",
        "# ===============================================\n",
        "# 12. STANDARD SCALING\n",
        "# ===============================================\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_res)\n",
        "\n",
        "# ===============================================\n",
        "# 13. TRAINâ€“VALIDATION SPLIT\n",
        "# ===============================================\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_scaled, y_res, test_size=0.2, random_state=42, stratify=y_res\n",
        ")\n",
        "\n",
        "# ===============================================\n",
        "# 14. LOGISTIC REGRESSION MODEL\n",
        "# ===============================================\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_val)\n",
        "\n",
        "# Metrics\n",
        "print(\"\\nLogistic Regression Results\")\n",
        "print(\"Accuracy :\", accuracy_score(y_val, y_pred))\n",
        "print(\"ROC-AUC :\", roc_auc_score(y_val, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_val, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred))\n",
        "\n",
        "# ===============================================\n",
        "# 15. SVM (Using Tuned Best Parameters)\n",
        "# ===============================================\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SVC(kernel='rbf', C=10, gamma=0.1, coef0=0.0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_val)\n",
        "\n",
        "print(\"\\nSVC Results\")\n",
        "print(\"Accuracy :\", accuracy_score(y_val, y_pred))\n",
        "print(\"ROC-AUC :\", roc_auc_score(y_val, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_val, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred))\n",
        "\n",
        "# ===============================================\n",
        "# 16. RANDOM FOREST (Best Tuned Parameters)\n",
        "# ===============================================\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=30,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features='sqrt'\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_val)\n",
        "\n",
        "print(\"\\nRandom Forest Results\")\n",
        "print(\"Accuracy :\", accuracy_score(y_val, y_pred))\n",
        "print(\"ROC-AUC :\", roc_auc_score(y_val, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_val, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred))\n",
        "\n",
        "# ===============================================\n",
        "# 17. TRAIN FINAL MODEL FOR PREDICTION\n",
        "# ===============================================\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=30,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features='sqrt',\n",
        "    oob_score=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf.fit(X_scaled, y_res)   # Train on full resampled dataset\n",
        "\n",
        "# ===============================================\n",
        "# 18. PROCESS TEST DATA\n",
        "# ===============================================\n",
        "test_df.info()\n",
        "\n",
        "test_ids = test_df['id']\n",
        "test_df.drop(['id','CustomerId','Surname'], axis=1, inplace=True)\n",
        "\n",
        "test_df.duplicated().sum()\n",
        "\n",
        "# Same encoding as training\n",
        "test_df = pd.get_dummies(test_df, columns=['Gender', 'Geography'],\n",
        "                         drop_first=True, dtype=int)\n",
        "\n",
        "# Scale test data\n",
        "X_test = scaler.transform(test_df)\n",
        "\n",
        "# ===============================================\n",
        "# 19. PREDICT TEST PROBABILITIES\n",
        "# ===============================================\n",
        "y_pred = rf.predict_proba(X_test)\n",
        "\n",
        "# ===============================================\n",
        "# 20. CREATE SUBMISSION FILE\n",
        "# ===============================================\n",
        "sub_df = pd.DataFrame({\n",
        "    'id': test_ids,\n",
        "    'Exited': y_pred[:,1]   # probability of exit\n",
        "})\n",
        "\n",
        "sub_df.to_csv('submission_Bank.csv', index=False, float_format='%.6f')\n",
        "\n",
        "print(\"\\nSubmission File Created Successfully!\")\n"
      ],
      "metadata": {
        "id": "NrLxmlfzUI9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# 1) IMPORT LIBRARIES\n",
        "# ======================================================\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_auc_score, confusion_matrix,\n",
        "    classification_report, roc_curve\n",
        ")\n",
        "import joblib\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(42)\n",
        "\n",
        "# ======================================================\n",
        "# 2) LOAD DATA (auto-detect CSV inside folder)\n",
        "# ======================================================\n",
        "input_path = \"/kaggle/input/health-heart-disease-data\"\n",
        "\n",
        "# If path is directory â†’ pick first CSV\n",
        "if os.path.isdir(input_path):\n",
        "    csv_file = glob.glob(os.path.join(input_path, \"*.csv\"))[0]\n",
        "else:\n",
        "    csv_file = input_path\n",
        "\n",
        "print(\"Reading:\", csv_file)\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# ======================================================\n",
        "# 3) BASIC EDA â€“ preview data\n",
        "# ======================================================\n",
        "print(\"\\n--- Data Head ---\")\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\n--- Describe ---\")\n",
        "display(df.describe().T)\n",
        "\n",
        "print(\"\\nTarget distribution:\")\n",
        "print(df['disease_risk'].value_counts())\n",
        "\n",
        "# ======================================================\n",
        "# 4) BOXPLOTS (Before Imputation)\n",
        "# ======================================================\n",
        "num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "num_cols.remove('disease_risk')\n",
        "\n",
        "plt.figure(figsize=(10, len(num_cols)*2))\n",
        "for i, col in enumerate(num_cols, 1):\n",
        "    plt.subplot(len(num_cols), 1, i)\n",
        "    plt.boxplot(df[col].dropna(), vert=False)\n",
        "    plt.title(f\"{col} (before imputation)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ======================================================\n",
        "# 5) HANDLE MISSING VALUES\n",
        "#    Numeric â†’ median, Categorical â†’ mode\n",
        "# ======================================================\n",
        "df_imputed = df.copy()\n",
        "\n",
        "for col in df_imputed.columns:\n",
        "    if df_imputed[col].isnull().sum() > 0:\n",
        "        if pd.api.types.is_numeric_dtype(df_imputed[col]):\n",
        "            df_imputed[col].fillna(df_imputed[col].median(), inplace=True)\n",
        "        else:\n",
        "            df_imputed[col].fillna(df_imputed[col].mode()[0], inplace=True)\n",
        "\n",
        "# ======================================================\n",
        "# 6) BOXPLOTS (After Imputation, Before Outliers)\n",
        "# ======================================================\n",
        "plt.figure(figsize=(10, len(num_cols)*2))\n",
        "for i, col in enumerate(num_cols, 1):\n",
        "    plt.subplot(len(num_cols), 1, i)\n",
        "    plt.boxplot(df_imputed[col], vert=False)\n",
        "    plt.title(f\"{col} (after imputation)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ======================================================\n",
        "# 7) OUTLIER REMOVAL USING IQR\n",
        "# ======================================================\n",
        "df_no_out = df_imputed.copy()\n",
        "\n",
        "for col in num_cols:\n",
        "    Q1 = df_no_out[col].quantile(0.25)\n",
        "    Q3 = df_no_out[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    df_no_out = df_no_out[(df_no_out[col] >= lower) & (df_no_out[col] <= upper)]\n",
        "\n",
        "# ======================================================\n",
        "# 8) BOXPLOTS (After Outlier Removal)\n",
        "# ======================================================\n",
        "plt.figure(figsize=(10, len(num_cols)*2))\n",
        "for i, col in enumerate(num_cols, 1):\n",
        "    plt.subplot(len(num_cols), 1, i)\n",
        "    plt.boxplot(df_no_out[col], vert=False)\n",
        "    plt.title(f\"{col} (after outlier removal)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save cleaned data\n",
        "df_no_out.to_csv(\"/kaggle/working/health_data_cleaned.csv\", index=False)\n",
        "\n",
        "# ======================================================\n",
        "# 9) TRAIN-TEST SPLIT + SCALING\n",
        "# ======================================================\n",
        "X = df_no_out.drop('disease_risk', axis=1)\n",
        "y = df_no_out['disease_risk']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ======================================================\n",
        "# 10) RANDOM FOREST + RandomizedSearchCV\n",
        "# ======================================================\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': [100,150,200,250,300],\n",
        "    'max_depth': [5,8,10,None],\n",
        "    'min_samples_split': [2,3,4,5,6,8,10],\n",
        "    'min_samples_leaf': [1,2,3,4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "rand_search = RandomizedSearchCV(\n",
        "    rf, param_dist, n_iter=20, cv=3,\n",
        "    n_jobs=-1, random_state=42, scoring='f1', verbose=1\n",
        ")\n",
        "\n",
        "rand_search.fit(X_train_scaled, y_train)\n",
        "best_rf = rand_search.best_estimator_\n",
        "\n",
        "# ======================================================\n",
        "# 11) TEST SET EVALUATION\n",
        "# ======================================================\n",
        "y_pred = best_rf.predict(X_test_scaled)\n",
        "y_proba = best_rf.predict_proba(X_test_scaled)[:,1]\n",
        "\n",
        "print(\"\\n--- Test Metrics ---\")\n",
        "print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall   :\", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score :\", f1_score(y_test, y_pred))\n",
        "print(\"AUC      :\", roc_auc_score(y_test, y_proba))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix Plot\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.imshow(cm)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(fpr, tpr)\n",
        "plt.plot([0,1],[0,1],'--')\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.show()\n",
        "\n",
        "# Feature Importance\n",
        "feat_imp = pd.Series(best_rf.feature_importances_, index=X.columns)\n",
        "display(feat_imp.sort_values(ascending=False))\n",
        "\n",
        "# ======================================================\n",
        "# 12) SAVE FINAL MODEL + SCALER\n",
        "# ======================================================\n",
        "joblib.dump(best_rf, \"/kaggle/working/rf_best_model.joblib\")\n",
        "joblib.dump(scaler, \"/kaggle/working/scaler.joblib\")\n"
      ],
      "metadata": {
        "id": "TAhJkGwFVVJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "#   COMPLETE ONE-CELL SOLUTION WITH COMMENTS + GRAPHS\n",
        "# ============================================================\n",
        "\n",
        "# -------------------- IMPORTS --------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "\n",
        "# -------------------- LOAD DATA --------------------\n",
        "df = pd.read_csv('/kaggle/input/ai-201-b-minor-mse-1/train_data.csv')\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "\n",
        "# -------------------- OUTLIER REMOVAL --------------------\n",
        "z_scores = (df - df.mean()) / df.std()\n",
        "df_clean = df[(np.abs(z_scores) <= 3).all(axis=1)]\n",
        "df = df_clean.copy()\n",
        "\n",
        "\n",
        "# -------------------- DATA VISUALIZATION --------------------\n",
        "plt.figure(figsize=(14, 6))\n",
        "for i, col in enumerate(df.columns[:-1]):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    sns.histplot(df[col], kde=True, bins=25)\n",
        "    plt.title(f\"Distribution of {col}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.histplot(df['Product_Strength_Deviation'], kde=True, bins=30, color='orange')\n",
        "plt.title(\"Distribution of Target\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# -------------------- MISSING VALUE HANDLING --------------------\n",
        "df['Machine_Speed_Deviation'] = df['Machine_Speed_Deviation'].fillna(df['Machine_Speed_Deviation'].mean())\n",
        "df['Pressure_Level_Deviation'] = df['Pressure_Level_Deviation'].fillna(df['Pressure_Level_Deviation'].mean())\n",
        "df['Temperature_Deviation'] = df['Temperature_Deviation'].fillna(df['Temperature_Deviation'].median())\n",
        "df['Product_Strength_Deviation'] = df['Product_Strength_Deviation'].fillna(df['Product_Strength_Deviation'].mean())\n",
        "\n",
        "\n",
        "# -------------------- TRAIN-TEST SPLIT --------------------\n",
        "X = df.drop('Product_Strength_Deviation', axis=1)\n",
        "y = df['Product_Strength_Deviation']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# -------------------- LINEAR REGRESSION --------------------\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_scaled, y)\n",
        "y_pred_lr = lr.predict(X_val)\n",
        "\n",
        "print(\"Linear Regression R2:\", r2_score(y_val, y_pred_lr))\n",
        "print(\"Linear Regression MSE:\", mean_squared_error(y_val, y_pred_lr))\n",
        "\n",
        "\n",
        "# -------------------- POLYNOMIAL REGRESSION --------------------\n",
        "degree = 7\n",
        "poly = PolynomialFeatures(degree=degree)\n",
        "X_poly = poly.fit_transform(X_scaled)\n",
        "\n",
        "X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(\n",
        "    X_poly, y, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "poly_model = LinearRegression()\n",
        "poly_model.fit(X_train_poly, y_train_poly)\n",
        "\n",
        "y_pred_poly = poly_model.predict(X_test_poly)\n",
        "\n",
        "print(f\"Polynomial Regression (Degree={degree}) R2:\", r2_score(y_test_poly, y_pred_poly))\n",
        "print(f\"Polynomial Regression (Degree={degree}) MSE:\", mean_squared_error(y_test_poly, y_pred_poly))\n",
        "\n",
        "\n",
        "# -------------------- DECISION TREE --------------------\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "param_grid_dt = {'max_depth': [2,4,6,8,10]}\n",
        "\n",
        "grid_dt = GridSearchCV(dt, param_grid_dt, cv=5, scoring='r2')\n",
        "grid_dt.fit(X_scaled, y)\n",
        "\n",
        "y_pred_dt = grid_dt.predict(X_val)\n",
        "\n",
        "print(\"Decision Tree R2:\", r2_score(y_val, y_pred_dt))\n",
        "print(\"Decision Tree MSE:\", mean_squared_error(y_val, y_pred_dt))\n",
        "print(\"Best DT Params:\", grid_dt.best_params_)\n",
        "\n",
        "\n",
        "# -------------------- KNN REGRESSION --------------------\n",
        "knn = KNeighborsRegressor()\n",
        "param_grid_knn = {\n",
        "    'n_neighbors': [3,5,7,9,11],\n",
        "    'weights': ['uniform','distance'],\n",
        "    'p': [1,2]\n",
        "}\n",
        "\n",
        "grid_knn = GridSearchCV(knn, param_grid_knn, cv=5, scoring='r2')\n",
        "grid_knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred_knn = grid_knn.predict(X_val)\n",
        "\n",
        "print(\"KNN R2:\", r2_score(y_val, y_pred_knn))\n",
        "print(\"KNN MSE:\", mean_squared_error(y_val, y_pred_knn))\n",
        "print(\"Best KNN Params:\", grid_knn.best_params_)\n",
        "\n",
        "\n",
        "# -------------------- PREDICT ON TEST FILE --------------------\n",
        "df_test = pd.read_csv('/kaggle/input/ai-201-b-minor-mse-1/test_data.csv')\n",
        "X_test = df_test.drop(columns=['id'])\n",
        "\n",
        "X_test_poly = poly.transform(scaler.transform(X_test))\n",
        "y_new_pred = poly_model.predict(X_test_poly)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'id': df_test['id'],\n",
        "    'Product_Strength_Deviation': y_new_pred\n",
        "})\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "submission.head()\n"
      ],
      "metadata": {
        "id": "maNLFTnSVwJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# ðŸ“¦ Multilinear Regression | Baseline Notebook\n",
        "# Evaluation: MSE & RÂ² Score\n",
        "# ==============================================\n",
        "\n",
        "# --- 1. Import Libraries ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "# --- 2. Load Data ---\n",
        "train = pd.read_csv('/content/train.csv')\n",
        "test = pd.read_csv('/content/test.csv')\n",
        "\n",
        "\n",
        "# --- 3. Quick Look ---\n",
        "print(\"Shape:\", train.shape)\n",
        "print(\"\\nFirst 5 rows:\\n\", train.head())\n",
        "\n",
        "\n",
        "# --- 4. Basic Info ---\n",
        "print(\"\\nData Info:\")\n",
        "print(train.info())\n",
        "\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(train.describe())\n",
        "\n",
        "\n",
        "# --- 5. Correlation Heatmap (to see feature relationships) ---\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(train.corr(numeric_only=True), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- 6. Pairplot (visualize patterns between features) ---\n",
        "sns.pairplot(train[['size','rooms','floor','age','price']], diag_kind='hist')\n",
        "plt.suptitle(\"Pairwise Feature Relationships\", y=1.02)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- 7. Prepare Data for Training ---\n",
        "# Independent variables (features)\n",
        "X = train[['size','rooms','floor','age']]\n",
        "\n",
        "# Target variable\n",
        "y = train['price']\n",
        "\n",
        "\n",
        "# --- 8. Train Linear Regression Model ---\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "\n",
        "# --- 9. Evaluate Model on Training Data ---\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "r2 = r2_score(y, y_pred)\n",
        "\n",
        "print(\"\\nâœ… Model Evaluation on Training Data:\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"RÂ² Score: {r2:.4f}\")\n",
        "\n",
        "\n",
        "# --- 10. Check Coefficients ---\n",
        "coeff_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Coefficient': model.coef_\n",
        "})\n",
        "\n",
        "print(\"\\nModel Coefficients:\")\n",
        "print(coeff_df)\n",
        "print(f\"Intercept: {model.intercept_:.2f}\")\n",
        "\n",
        "\n",
        "# --- 11. Predict on Test Data ---\n",
        "test_pred = model.predict(test[['size','rooms','floor','age']])\n",
        "\n",
        "\n",
        "# --- 12. Create Submission File ---\n",
        "submission = pd.DataFrame({\n",
        "    'id': test['id'],\n",
        "    'price': test_pred.round(2)     # round to 2 decimals\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"\\nâœ… Clean Submission Preview:\")\n",
        "print(submission.to_string(index=False))\n",
        "\n",
        "\n",
        "# --- 13. Optional: Actual vs Predicted Plot ---\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(y, y_pred, alpha=0.7, color='green')\n",
        "plt.xlabel(\"Actual Price\")\n",
        "plt.ylabel(\"Predicted Price\")\n",
        "plt.title(\"Actual vs Predicted Price\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "usVQTuFFWxZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# ðŸ“Œ COMPLETE MACHINE LEARNING PIPELINE (RF + HyperTuning + EDA)\n",
        "# ======================================================================\n",
        "\n",
        "# 1ï¸âƒ£ IMPORT LIBRARIES\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "# ======================================================================\n",
        "# 2ï¸âƒ£ HOW TO CREATE A CSV FILE (Example)\n",
        "# ======================================================================\n",
        "data = {\n",
        "    \"Pclass\": [1, 3, 2],\n",
        "    \"Sex\": [\"male\", \"female\", \"male\"],\n",
        "    \"Age\": [22, 28, np.nan],\n",
        "    \"Fare\": [71.2, 7.25, 12.85],\n",
        "    \"Survived\": [0, 1, 1]\n",
        "}\n",
        "\n",
        "demo_df = pd.DataFrame(data)\n",
        "demo_df.to_csv(\"sample.csv\", index=False)\n",
        "print(\"Sample CSV created: sample.csv\\n\")\n",
        "\n",
        "# ======================================================================\n",
        "# 3ï¸âƒ£ LOAD DATA\n",
        "# ======================================================================\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Titanic-Dataset.csv\")\n",
        "\n",
        "print(\"First 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# ======================================================================\n",
        "# 4ï¸âƒ£ REMOVE NULL VALUES (multiple ways)\n",
        "# ======================================================================\n",
        "\n",
        "# â— Remove rows where target is null\n",
        "df = df.dropna(subset=['Survived'])\n",
        "\n",
        "# â— Fill numeric missing values with median\n",
        "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
        "\n",
        "# â— Fill categorical missing values with mode\n",
        "cat_cols = df.select_dtypes(include=['object']).columns\n",
        "df[cat_cols] = df[cat_cols].fillna(df[cat_cols].mode().iloc[0])\n",
        "\n",
        "print(\"\\nNull values removed.\\n\")\n",
        "\n",
        "# ======================================================================\n",
        "# 5ï¸âƒ£ EDA â†’ DISTRIBUTION PLOTS\n",
        "# ======================================================================\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.histplot(df['Age'], kde=True)\n",
        "plt.title(\"Age Distribution\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(data=df, x='Survived')\n",
        "plt.title(\"Survived Count\")\n",
        "plt.show()\n",
        "\n",
        "# Correlation heatmap\n",
        "plt.figure(figsize=(7,6))\n",
        "sns.heatmap(df.corr(numeric_only=True), annot=True, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# ======================================================================\n",
        "# 6ï¸âƒ£ OUTLIER CAPPING (YOUR CODE) â€” CATEGORY WISE\n",
        "# ======================================================================\n",
        "\n",
        "def cap_outliers_categorywise_all(df, cat_col, num_cols):\n",
        "    def cap_group(group):\n",
        "        group = group.copy()\n",
        "        for col in num_cols:\n",
        "            Q1 = group[col].quantile(0.25)\n",
        "            Q3 = group[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower = Q1 - 1.5 * IQR\n",
        "            upper = Q3 + 1.5 * IQR\n",
        "            group[col] = group[col].clip(lower, upper)  # Cap values\n",
        "        return group\n",
        "\n",
        "    df_capped = df.groupby(cat_col, group_keys=False, observed=True, sort=False).apply(cap_group)\n",
        "    return df_capped\n",
        "\n",
        "# Apply outlier capping using \"Pclass\" as category\n",
        "numeric_cols = ['Age', 'Fare']\n",
        "df = cap_outliers_categorywise_all(df, 'Pclass', numeric_cols)\n",
        "\n",
        "print(\"\\nOutliers capped category-wise.\\n\")\n",
        "\n",
        "# ======================================================================\n",
        "# 7ï¸âƒ£ LABEL ENCODING (with INVERSE TRANSFORM support)\n",
        "# ======================================================================\n",
        "\n",
        "le_sex = LabelEncoder()\n",
        "le_embarked = LabelEncoder()\n",
        "\n",
        "df['Sex'] = le_sex.fit_transform(df['Sex'])\n",
        "df['Embarked'] = le_embarked.fit_transform(df['Embarked'])\n",
        "\n",
        "print(\"\\nLabel Encoding Completed.\\n\")\n",
        "\n",
        "# To decode later:\n",
        "# original_value = le_sex.inverse_transform([encoded_value])\n",
        "\n",
        "# ======================================================================\n",
        "# 8ï¸âƒ£ FEATURE SELECTION\n",
        "# ======================================================================\n",
        "\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "X = df[features]\n",
        "y = df['Survived']\n",
        "\n",
        "# ======================================================================\n",
        "# 9ï¸âƒ£ TRAIN/TEST SPLIT\n",
        "# ======================================================================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ======================================================================\n",
        "# ðŸ”Ÿ RANDOM FOREST (BASE MODEL)\n",
        "# ======================================================================\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# ======================================================================\n",
        "# 1ï¸âƒ£1ï¸âƒ£ HYPERPARAMETER TUNING â€” GRID SEARCH\n",
        "# ======================================================================\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [5, 10, None],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "print(\"\\nBest Parameters:\", grid.best_params_)\n",
        "\n",
        "# ======================================================================\n",
        "# 1ï¸âƒ£2ï¸âƒ£ PREDICT\n",
        "# ======================================================================\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_prob = best_model.predict_proba(X_test)[:,1]  # For AUC\n",
        "\n",
        "# ======================================================================\n",
        "# 1ï¸âƒ£3ï¸âƒ£ EVALUATION METRICS\n",
        "# ======================================================================\n",
        "\n",
        "print(\"\\n================= MODEL METRICS =================\")\n",
        "print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall   :\", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score :\", f1_score(y_test, y_pred))\n",
        "print(\"ROC-AUC  :\", roc_auc_score(y_test, y_prob))\n",
        "\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# ======================================================================\n",
        "# 1ï¸âƒ£4ï¸âƒ£ INVERSE TRANSFORM EXAMPLE\n",
        "# ======================================================================\n",
        "encoded_val = df['Sex'].iloc[0]\n",
        "decoded_val = le_sex.inverse_transform([encoded_val])[0]\n",
        "\n",
        "print(\"\\nEncoded Sex =\", encoded_val, \" â†’ Decoded =\", decoded_val)\n"
      ],
      "metadata": {
        "id": "5CmYGZAycCOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================\n",
        "# ðŸ“Œ COMPLETE PIPELINE WITH OUTLIER ANALYSIS + ONE-HOT + RANDOM FOREST\n",
        "# =====================================================================\n",
        "\n",
        "# 1ï¸âƒ£ Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, roc_auc_score, log_loss, confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 2ï¸âƒ£ Load Data\n",
        "# =====================================================================\n",
        "train = pd.read_csv(\"/kaggle/input/mock-test-2-mse-2/train.csv\")\n",
        "test = pd.read_csv(\"/kaggle/input/mock-test-2-mse-2/test.csv\")\n",
        "\n",
        "print(train.head())\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 3ï¸âƒ£ Split Target + Features\n",
        "# =====================================================================\n",
        "y = train[\"Status\"]\n",
        "X = train.drop(columns=[\"Status\"])\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 4ï¸âƒ£ Handle Missing Values (Safe Method)\n",
        "# =====================================================================\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == 'object':\n",
        "        # Categorical â†’ fill with mode\n",
        "        X[col] = X[col].fillna(X[col].mode()[0])\n",
        "        test[col] = test[col].fillna(test[col].mode()[0])\n",
        "    else:\n",
        "        # Numerical â†’ fill with mean\n",
        "        X[col] = X[col].fillna(X[col].mean())\n",
        "        test[col] = test[col].fillna(test[col].mean())\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 5ï¸âƒ£ Outlier Analysis\n",
        "# =====================================================================\n",
        "print(\"\\n================ OUTLIER ANALYSIS ================\")\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "\n",
        "# ------------------------- (A) Isolation Forest ------------------------\n",
        "iso = IsolationForest(contamination=0.03, random_state=42)\n",
        "outlier_flags = iso.fit_predict(X[numeric_cols])\n",
        "\n",
        "# -1 = outlier, 1 = normal\n",
        "X_clean = X[outlier_flags == 1]\n",
        "y_clean = y[outlier_flags == 1]\n",
        "\n",
        "print(f\"Rows before IsolationForest: {len(X)}\")\n",
        "print(f\"Rows after cleaning: {len(X_clean)}\")\n",
        "\n",
        "\n",
        "# ------------------------- (B) IQR Outlier Capping ----------------------\n",
        "# You asked to include this function\n",
        "\n",
        "def cap_outliers_categorywise_all(df, cat_col, num_cols):\n",
        "    \"\"\"\n",
        "    Caps outliers within each categorical group using IQR method.\n",
        "    \"\"\"\n",
        "    def cap_group(group):\n",
        "        group = group.copy()\n",
        "        for col in num_cols:\n",
        "            Q1 = group[col].quantile(0.25)\n",
        "            Q3 = group[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower = Q1 - 1.5 * IQR\n",
        "            upper = Q3 + 1.5 * IQR\n",
        "            group[col] = group[col].clip(lower, upper)\n",
        "        return group\n",
        "\n",
        "    return df.groupby(cat_col, group_keys=False, observed=True, sort=False).apply(cap_group)\n",
        "\n",
        "categorical_col_for_capping = \"Region\" if \"Region\" in X_clean.columns else X_clean.select_dtypes(include=\"object\").columns[0]\n",
        "\n",
        "X_capped = cap_outliers_categorywise_all(X_clean.join(y_clean), categorical_col_for_capping, numeric_cols)\n",
        "\n",
        "# Separate target back\n",
        "y_clean = X_capped[\"Status\"]\n",
        "X_clean = X_capped.drop(columns=[\"Status\"])\n",
        "\n",
        "print(\"Outlier capping complete!\")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 6ï¸âƒ£ Trainâ€“Validation Split (after outlier cleaning)\n",
        "# =====================================================================\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_clean, y_clean, test_size=0.2, random_state=42, stratify=y_clean\n",
        ")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 7ï¸âƒ£ Preprocessing (OneHotEncoding for categorical)\n",
        "# =====================================================================\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
        "        (\"num\", \"passthrough\", numeric_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 8ï¸âƒ£ RF Pipeline\n",
        "# =====================================================================\n",
        "base_model = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 9ï¸âƒ£ Hyperparameter Tuning\n",
        "# =====================================================================\n",
        "param_grid = {\n",
        "    \"classifier__n_estimators\": [100, 200],\n",
        "    \"classifier__max_depth\": [10, 20, None],\n",
        "    \"classifier__min_samples_split\": [2, 5],\n",
        "    \"classifier__min_samples_leaf\": [1, 2]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    base_model,\n",
        "    param_grid,\n",
        "    scoring=\"neg_log_loss\",\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "print(\"\\nðŸ”Ž Best Parameters:\")\n",
        "print(grid.best_params_)\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# ðŸ”Ÿ Validation Results\n",
        "# =====================================================================\n",
        "y_pred = best_model.predict(X_val)\n",
        "y_proba = best_model.predict_proba(X_val)\n",
        "\n",
        "print(\"\\n================ VALIDATION METRICS ================\")\n",
        "print(\"Accuracy :\", accuracy_score(y_val, y_pred))\n",
        "print(\"Precision:\", precision_score(y_val, y_pred, average=\"weighted\"))\n",
        "print(\"Recall   :\", recall_score(y_val, y_pred, average=\"weighted\"))\n",
        "print(\"F1 Score :\", f1_score(y_val, y_pred, average=\"weighted\"))\n",
        "print(\"Log Loss :\", log_loss(y_val, y_proba))\n",
        "\n",
        "try:\n",
        "    print(\"AUC Score:\", roc_auc_score(y_val, y_proba, multi_class=\"ovr\"))\n",
        "except:\n",
        "    print(\"AUC Score: multiclass AUC failed\")\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_val, y_pred))\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 1ï¸âƒ£1ï¸âƒ£ Prepare Test Data (same columns as training)\n",
        "# =====================================================================\n",
        "test_processed = test[X.columns]   # aligns column order\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 1ï¸âƒ£2ï¸âƒ£ Submission File\n",
        "# =====================================================================\n",
        "predict_proba = best_model.predict_proba(test_processed)\n",
        "\n",
        "class_labels = list(best_model.classes_)\n",
        "\n",
        "submission = pd.DataFrame({\"id\": test[\"id\"]})\n",
        "\n",
        "for i, cls in enumerate(class_labels):\n",
        "    submission[f\"Status_{cls}\"] = predict_proba[:, i]\n",
        "\n",
        "submission.to_csv(\"Ayush.csv\", index=False)\n",
        "\n",
        "print(\"\\nðŸ“ Submission Preview:\")\n",
        "print(submission.head())\n"
      ],
      "metadata": {
        "id": "hOi-04F1lmbS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}